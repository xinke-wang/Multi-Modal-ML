# Multi-Modal-ML
This repo would be occasionally but continuously updated, which would collect papers that are related to Multi-Modal Machine Learning applications, especially those modals that are **NOT** well learned at this point, that is to say, common and popular tasks such as Visual Question Answering, Image Caption Generation will not be focused here.

## Fancy Applications
*This part collects fancy applications that rely on Multi-Modal Machine Learning. If only one modality is involed, it means this paper introduces how to obatin the data under this modal.*

| Year | Venue | Paper | Modalities | Project/Code |
|------|-------|-------|------------|--------------|
|2019|CVPR|[Connecting Touch and Vision via Cross-Modal Prediction](https://arxiv.org/pdf/1906.06322.pdf) |Vision+Touch|[Project](http://stag.csail.mit.edu/)/[Code](https://github.com/Erkil1452/touch) |
|2019|Nature|[Learning the signatures of the human grasp using a scalable tactile glove](https://www.nature.com/articles/s41586-019-1234-z) |Touch|[Project](http://visgel.csail.mit.edu/)/[Code](https://github.com/YunzhuLi/VisGel) |
| 2019 |IJCV| [Learning Sight from Sound:Ambient Sound Provides Supervision for Visual Learning](https://arxiv.org/pdf/1712.07271.pdf)|Vision+Sound|-|
| 2016 |ECCV|[Ambient Sound Provides Supervisionfor Visual Learning](https://arxiv.org/pdf/1608.07017.pdf) |Vision+Sound|-|
|2016|CVPR| [Visually Indicated Sounds](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Owens_Visually_Indicated_Sounds_CVPR_2016_paper.pdf) | Vision+Sound | [Project](http://andrewowens.com/vis/index.html) |


## Employing Extra Modalities to Boost Traditional Tasks
*This part collects the papers that introduce new modalities into traditional Tasks.*

| Year | Venue | Paper | Task | Basic | New |
|------|-------|-------|------|-------------|-----------|
|2019|ICCV|[Self-Supervised Moving Vehicle Tracking With Stereo Sound](http://openaccess.thecvf.com/content_ICCV_2019/papers/Gan_Self-Supervised_Moving_Vehicle_Tracking_With_Stereo_Sound_ICCV_2019_paper.pdf)|Tracking|Vision|Sound|
|2019|CVPRW|[WiFi and Vision Multimodal Learning for Accurate and Robust Device-Free Human Activity Recognition](http://openaccess.thecvf.com/content_CVPRW_2019/html/MULA/Zou_WiFi_and_Vision_Multimodal_Learning_for_Accurate_and_Robust_Device-Free_CVPRW_2019_paper.html)|Recognition|Vision|WiFi|
|      |       |       |      |             |           |


## Tutorial/Workshop/Survey
*This part lists some extra resources about Multi-modal Machine Learning*

**Survey Papers**
| Year | Venue | Title |
|------|-------|-------|
|2018|T-PAMI|[Multimodal Machine Learning: A Survey and Taxonomy](https://arxiv.org/pdf/1705.09406.pdf)|

**Workshops**
| Year | Venue | Title |
|------|-------|-------|
|2020|CVPR|[Workshop on Multimodal Learning](https://mul-workshop.github.io/)|
|2019|ICCV|[Cross-Modal Learning in Real World](https://cromol.github.io/)|
|2019|CVPR|[2nd Multimodal Learning and Applications Workshop (MULA)](https://mula-workshop.github.io/)|
|2018|ECCV|[1st Multimodal Learning and Applications Workshop (MULA)](https://mula2018.github.io/)|


## People
*This part lists researchers that are actively working on Multi-Modal Machine Learning.*
| Name | Affiliation | Research Interests | Google Scholar |
|------|-------------|--------------------| -------------- |
| [Antonio Torralba](http://web.mit.edu/torralba/www/) | MIT | Vision+Audition+Touch | [Scholar](https://scholar.google.com/citations?user=8cxDHS4AAAAJ) |
|      |             |                    | |
|      |             |                    | |

## Contact
If you are also interested in Multi-Modal Machine Learning, and would like to recommend some papers/projects to this repo, feel free to open an issue or make a pull request.
