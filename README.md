# Multi-Modal-ML
This repo would be occasionally but continuously updated, which would collect papers that are related to Multi-Modal Machine Learning applications.

## Fancy Applications
*This part collects fancy applications that rely on Multi-Modal Machine Learning, especially those modal combinations that are **NOT** well learned at this point, that is to say, common and popular tasks such as Visual Question Answering, Image Caption Generation will not be focused here. If only one modality is involed, it means this paper introduces how to obatin the data under this modal.*

| Year | Venue | Paper | Modalities | Project/Code |
|------|-------|-------|------------|--------------|
|2020|ECCV|[Multiple Sound Sources Localization fromCoarse to Fine](https://arxiv.org/pdf/2007.06355.pdf) |Vision+Sound|-|
|2020|CVPR|[Music Gesture for Visual Sound Separation](https://arxiv.org/pdf/2004.09476.pdf) |Vision+Sound|[Project](http://music-gesture.csail.mit.edu/)|
|2020|ICASSP|[Sight to Sound: An End-to-end Approach for Visual Piano Transcription](http://www.robots.ox.ac.uk/~vgg/publications/2020/Koepke20/koepke20.pdf)|Vision+Sound|[Project](https://www.robots.ox.ac.uk/~vgg/research/sighttosound/)|
|2019|CVPR|[Connecting Touch and Vision via Cross-Modal Prediction](https://arxiv.org/pdf/1906.06322.pdf) |Vision+Touch|[Project](http://stag.csail.mit.edu/)/[Code](https://github.com/Erkil1452/touch) |
|2019|Nature|[Learning the signatures of the human grasp using a scalable tactile glove](https://www.nature.com/articles/s41586-019-1234-z) |Touch|[Project](http://visgel.csail.mit.edu/)/[Code](https://github.com/YunzhuLi/VisGel) |
|2019|IJCV|[Learning Sight from Sound:Ambient Sound Provides Supervision for Visual Learning](https://arxiv.org/pdf/1712.07271.pdf)|Vision+Sound|-|
|2019|NC|[Real-time decoding of question-and-answer speech dialogue using human cortical activity](https://www.nature.com/articles/s41467-019-10994-4)|Speech+ECoG|-|
|2017|CVPR|[Lip Reading Sentences in the Wild](http://www.robots.ox.ac.uk:5000/~vgg/publications/2017/Chung17/chung17.pdf)|Vision+Speech|-|
|2016|ECCV|[Ambient Sound Provides Supervisionfor Visual Learning](https://arxiv.org/pdf/1608.07017.pdf) |Vision+Sound|-|
|2016|CVPR| [Visually Indicated Sounds](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Owens_Visually_Indicated_Sounds_CVPR_2016_paper.pdf) |Vision+Sound| [Project](http://andrewowens.com/vis/index.html) |


## Employing Extra Modalities to Boost Traditional Tasks
*This part collects the papers that introduce new modalities into traditional Tasks.*

| Year | Venue | Paper | Task | Basic | New |
|------|-------|-------|------|-------------|-----------|
|2020|ECCV|[Not only Look, but also Listen: LearningMultimodal Violence Detection under WeakSupervision](https://arxiv.org/pdf/2007.04687.pdf)|Detection|Vision|Sound|
|2019|ICCV|[Self-Supervised Moving Vehicle Tracking With Stereo Sound](http://openaccess.thecvf.com/content_ICCV_2019/papers/Gan_Self-Supervised_Moving_Vehicle_Tracking_With_Stereo_Sound_ICCV_2019_paper.pdf)|Tracking|Vision|Sound|
|2019|ICCVW|[DECCNet: Depth Enhanced Crowd Counting](http://openaccess.thecvf.com/content_ICCVW_2019/papers/CROMOL/Yang_DECCNet_Depth_Enhanced_Crowd_Counting_ICCVW_2019_paper.pdf)|Counting|Vision|Depth|
|2019|CVPRW|[WiFi and Vision Multimodal Learning for Accurate and Robust Device-Free Human Activity Recognition](http://openaccess.thecvf.com/content_CVPRW_2019/html/MULA/Zou_WiFi_and_Vision_Multimodal_Learning_for_Accurate_and_Robust_Device-Free_CVPRW_2019_paper.html)|Recognition|Vision|WiFi|


## Datasets
*This part lists some huge datasets that include multi-modal annotations*

| Year | Dataset | Modalities | Project | Paper |
|------|---------|------------|---------|-------|
|2020|VGG-Sound|Vision+Sound|[Project](http://www.robots.ox.ac.uk/~vgg/data/vggsound/)|[ICASSP](https://www.robots.ox.ac.uk/~vgg/publications/2020/Chen20/chen20.pdf)|
|2017|Lip Reading in the Wild|Vision+Speech|[Project](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html)|[ACCV](https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16/chung16.pdf)|
|2016|Cross-Modal Places|Vision+Language|[Project](http://projects.csail.mit.edu/cmplaces/)|[CVPR](http://cmplaces.csail.mit.edu/content/paper.pdf) & [T-PAMI](http://cmplaces.csail.mit.edu/content/paper_pami.pdf)|


## Tutorial/Workshop/Survey
*This part lists some extra resources about Multi-modal Machine Learning*

**Survey Papers**
| Year | Venue | Title |
|------|-------|-------|
|2018|T-PAMI|[Multimodal Machine Learning: A Survey and Taxonomy](https://arxiv.org/pdf/1705.09406.pdf)|

**Workshops**
| Year | Venue | Title | Proceedings |
|------|-------|-------| ----------- |
|2020|CVPR|[Workshop on Multimodal Learning](https://mul-workshop.github.io/)|Proceedings|
|2019|ICCV|[Cross-Modal Learning in Real World](https://cromol.github.io/)|[Proceedings](http://openaccess.thecvf.com/ICCV2019_workshops/ICCV2019_CROMOL.py)|
|2019|CVPR|[2nd Multimodal Learning and Applications Workshop (MULA)](https://mula-workshop.github.io/)|[Proceedings](http://openaccess.thecvf.com/CVPR2019_workshops/CVPR2019_MULA.py)|
|2018|ECCV|[1st Multimodal Learning and Applications Workshop (MULA)](https://mula2018.github.io/)|[Proceedings](http://openaccess.thecvf.com/ECCV2018_workshops/ECCV2018_W35.py)|

**Tutorials**
| Year | Venue | Title |
|------|-------|-------|
|2016|CVPR|[Multimodal Machine Learning tutorial](https://sites.google.com/site/multiml2016cvpr/)|


## People
*This part lists researchers that are actively working on Multi-Modal Machine Learning.*
| Name | Affiliation | Research Interests | Google Scholar |
|------|-------------|--------------------| -------------- |
|[Antonio Torralba](http://web.mit.edu/torralba/www/) |MIT|Vision+Audition+Touch|[Scholar](https://scholar.google.com/citations?user=8cxDHS4AAAAJ) |
|[Andrew Zisserman](https://www.robots.ox.ac.uk/~az/)|Oxford|Vision+Audio|[Scholar](https://scholar.google.co.uk/citations?user=UZ5wscMAAAAJ)|
|[Andrea Vedaldi](http://www.robots.ox.ac.uk/~vedaldi/)|Oxford|Vision+Audio|[Scholar](https://scholar.google.com.au/citations?user=bRT7t28AAAAJ)|

## Contact
If you are also interested in Multi-Modal Machine Learning, and would like to recommend some papers/projects to this repo, feel free to open an issue or make a pull request.
